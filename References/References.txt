1:  tinyllama-an-open-source-small-language-model : https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model
2:  MedAlpaca: https://arxiv.org/pdf/2304.08247.pdf, https://huggingface.co/medalpaca
3:  Carbon Footprint of LLMs: https://arxiv.org/pdf/2211.02001.pdf, https://arxiv.org/pdf/2309.14393.pdf
4:  On XAI for LLMs:  https://arxiv.org/pdf/2402.01761.pdf , https://arxiv.org/pdf/2309.01029.pdf
5:  “The cost of understanding”, C. Jean-Quartier et al. 2023: https://www.mdpi.com/2079-3197/11/5/92
6:  https://github.com/prp-e/nucleus
7:  https://arxiv.org/abs/2009.07118
8:  https://aclanthology.org/2022.bigscience-1.8/
9:  https://github.com/karpathy/nanoGPT
10: Reference tinyllama chat assistant: https://farmaker47.medium.com/train-a-tiny-llama-model-to-help-on-a-specific-domain-task-9fc877573253
11: TinyLlamas models pretrained: https://github.com/karpathy/llama2.c#models
12: Train tinyllama and stable lm2: https://medium.com/@geronimo7/tinyllama-1-1b-and-stable-lm-2-1-6b-cc0051d79be9
13: Train new dataset on tinyllama video: https://www.youtube.com/watch?v=OVqe6GTrDFM
14: unsloth for faster training : https://github.com/unslothai/unsloth
15: How to use unsloth: https://www.fahdmirza.com/2024/01/train-tinyllama-11b-locally-on-own.html
16: Train Tinyllama on Taylor Swift Lyrics: https://www.youtube.com/watch?v=3SlpXBvIqNw