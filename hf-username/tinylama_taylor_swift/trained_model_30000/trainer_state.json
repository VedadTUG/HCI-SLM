{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 59.347181008902076,
  "eval_steps": 500,
  "global_step": 30000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.19782393669634027,
      "grad_norm": 1.2017133235931396,
      "learning_rate": 0.0002222222222222222,
      "loss": 3.0511,
      "step": 100
    },
    {
      "epoch": 0.39564787339268054,
      "grad_norm": 0.9761494398117065,
      "learning_rate": 0.0004444444444444444,
      "loss": 2.8251,
      "step": 200
    },
    {
      "epoch": 0.5934718100890207,
      "grad_norm": 0.8211073279380798,
      "learning_rate": 0.0006666666666666666,
      "loss": 2.8082,
      "step": 300
    },
    {
      "epoch": 0.7912957467853611,
      "grad_norm": 0.8607735633850098,
      "learning_rate": 0.0008888888888888888,
      "loss": 2.7599,
      "step": 400
    },
    {
      "epoch": 0.9891196834817013,
      "grad_norm": 0.881384551525116,
      "learning_rate": 0.0011111111111111111,
      "loss": 2.7881,
      "step": 500
    },
    {
      "epoch": 1.1869436201780414,
      "grad_norm": 0.9484056234359741,
      "learning_rate": 0.0013333333333333333,
      "loss": 2.7202,
      "step": 600
    },
    {
      "epoch": 1.3847675568743818,
      "grad_norm": 0.9536575078964233,
      "learning_rate": 0.0015555555555555557,
      "loss": 2.7427,
      "step": 700
    },
    {
      "epoch": 1.582591493570722,
      "grad_norm": 1.092895269393921,
      "learning_rate": 0.0017777777777777776,
      "loss": 2.8013,
      "step": 800
    },
    {
      "epoch": 1.7804154302670623,
      "grad_norm": 1.0745056867599487,
      "learning_rate": 0.002,
      "loss": 2.8051,
      "step": 900
    },
    {
      "epoch": 1.9782393669634026,
      "grad_norm": 1.1631577014923096,
      "learning_rate": 0.0019999417253661234,
      "loss": 2.8535,
      "step": 1000
    },
    {
      "epoch": 2.1760633036597428,
      "grad_norm": 1.2994120121002197,
      "learning_rate": 0.0019997669082563595,
      "loss": 2.6969,
      "step": 1100
    },
    {
      "epoch": 2.373887240356083,
      "grad_norm": 1.1249219179153442,
      "learning_rate": 0.0019994755690455153,
      "loss": 2.7404,
      "step": 1200
    },
    {
      "epoch": 2.5717111770524235,
      "grad_norm": 1.4460450410842896,
      "learning_rate": 0.0019990677416889605,
      "loss": 2.773,
      "step": 1300
    },
    {
      "epoch": 2.7695351137487636,
      "grad_norm": 1.2777079343795776,
      "learning_rate": 0.001998543473718677,
      "loss": 2.7769,
      "step": 1400
    },
    {
      "epoch": 2.9673590504451037,
      "grad_norm": 1.1474076509475708,
      "learning_rate": 0.0019979028262377117,
      "loss": 2.7801,
      "step": 1500
    },
    {
      "epoch": 3.1651829871414443,
      "grad_norm": 1.3340996503829956,
      "learning_rate": 0.0019971458739130596,
      "loss": 2.6266,
      "step": 1600
    },
    {
      "epoch": 3.3630069238377844,
      "grad_norm": 1.5032126903533936,
      "learning_rate": 0.00199627270496696,
      "loss": 2.6362,
      "step": 1700
    },
    {
      "epoch": 3.5608308605341246,
      "grad_norm": 1.2607015371322632,
      "learning_rate": 0.0019952834211666138,
      "loss": 2.6862,
      "step": 1800
    },
    {
      "epoch": 3.7586547972304647,
      "grad_norm": 1.4951363801956177,
      "learning_rate": 0.001994178137812324,
      "loss": 2.703,
      "step": 1900
    },
    {
      "epoch": 3.9564787339268053,
      "grad_norm": 1.3343517780303955,
      "learning_rate": 0.0019929569837240564,
      "loss": 2.742,
      "step": 2000
    },
    {
      "epoch": 4.154302670623146,
      "grad_norm": 1.4341098070144653,
      "learning_rate": 0.001991620101226425,
      "loss": 2.5524,
      "step": 2100
    },
    {
      "epoch": 4.3521266073194855,
      "grad_norm": 1.3353004455566406,
      "learning_rate": 0.0019901676461321067,
      "loss": 2.5962,
      "step": 2200
    },
    {
      "epoch": 4.549950544015826,
      "grad_norm": 1.1922417879104614,
      "learning_rate": 0.0019885997877236786,
      "loss": 2.6033,
      "step": 2300
    },
    {
      "epoch": 4.747774480712166,
      "grad_norm": 1.5672038793563843,
      "learning_rate": 0.0019869167087338906,
      "loss": 2.6596,
      "step": 2400
    },
    {
      "epoch": 4.945598417408506,
      "grad_norm": 1.3134069442749023,
      "learning_rate": 0.0019851186053243667,
      "loss": 2.6608,
      "step": 2500
    },
    {
      "epoch": 5.143422354104847,
      "grad_norm": 1.5080316066741943,
      "learning_rate": 0.001983205687062742,
      "loss": 2.5376,
      "step": 2600
    },
    {
      "epoch": 5.341246290801187,
      "grad_norm": 1.4352151155471802,
      "learning_rate": 0.0019811781768982392,
      "loss": 2.5107,
      "step": 2700
    },
    {
      "epoch": 5.539070227497527,
      "grad_norm": 1.5692847967147827,
      "learning_rate": 0.0019790363111356836,
      "loss": 2.5581,
      "step": 2800
    },
    {
      "epoch": 5.736894164193868,
      "grad_norm": 1.602339267730713,
      "learning_rate": 0.0019767803394079614,
      "loss": 2.6009,
      "step": 2900
    },
    {
      "epoch": 5.9347181008902075,
      "grad_norm": 1.5051637887954712,
      "learning_rate": 0.001974410524646926,
      "loss": 2.6188,
      "step": 3000
    },
    {
      "epoch": 6.132542037586548,
      "grad_norm": 1.3936299085617065,
      "learning_rate": 0.001971927143052752,
      "loss": 2.4778,
      "step": 3100
    },
    {
      "epoch": 6.330365974282889,
      "grad_norm": 1.6007449626922607,
      "learning_rate": 0.0019693304840617456,
      "loss": 2.4914,
      "step": 3200
    },
    {
      "epoch": 6.528189910979228,
      "grad_norm": 1.4991363286972046,
      "learning_rate": 0.001966620850312611,
      "loss": 2.5219,
      "step": 3300
    },
    {
      "epoch": 6.726013847675569,
      "grad_norm": 1.6826368570327759,
      "learning_rate": 0.001963798557611178,
      "loss": 2.5475,
      "step": 3400
    },
    {
      "epoch": 6.9238377843719086,
      "grad_norm": 1.7928725481033325,
      "learning_rate": 0.0019608639348935937,
      "loss": 2.5863,
      "step": 3500
    },
    {
      "epoch": 7.121661721068249,
      "grad_norm": 1.811753273010254,
      "learning_rate": 0.001957817324187987,
      "loss": 2.4388,
      "step": 3600
    },
    {
      "epoch": 7.31948565776459,
      "grad_norm": 1.6860558986663818,
      "learning_rate": 0.0019546590805746052,
      "loss": 2.4123,
      "step": 3700
    },
    {
      "epoch": 7.517309594460929,
      "grad_norm": 1.6365197896957397,
      "learning_rate": 0.0019513895721444286,
      "loss": 2.4973,
      "step": 3800
    },
    {
      "epoch": 7.71513353115727,
      "grad_norm": 1.8654212951660156,
      "learning_rate": 0.0019480091799562705,
      "loss": 2.4831,
      "step": 3900
    },
    {
      "epoch": 7.9129574678536105,
      "grad_norm": 1.5166717767715454,
      "learning_rate": 0.0019445182979923655,
      "loss": 2.5652,
      "step": 4000
    },
    {
      "epoch": 8.11078140454995,
      "grad_norm": 1.669871211051941,
      "learning_rate": 0.0019409173331124499,
      "loss": 2.454,
      "step": 4100
    },
    {
      "epoch": 8.308605341246292,
      "grad_norm": 1.7826646566390991,
      "learning_rate": 0.0019372067050063438,
      "loss": 2.4096,
      "step": 4200
    },
    {
      "epoch": 8.506429277942631,
      "grad_norm": 1.5939809083938599,
      "learning_rate": 0.0019333868461450358,
      "loss": 2.4186,
      "step": 4300
    },
    {
      "epoch": 8.704253214638971,
      "grad_norm": 1.6639293432235718,
      "learning_rate": 0.0019294582017302796,
      "loss": 2.446,
      "step": 4400
    },
    {
      "epoch": 8.90207715133531,
      "grad_norm": 1.7423067092895508,
      "learning_rate": 0.0019254212296427042,
      "loss": 2.4921,
      "step": 4500
    },
    {
      "epoch": 9.099901088031652,
      "grad_norm": 1.6259005069732666,
      "learning_rate": 0.001921276400388451,
      "loss": 2.4036,
      "step": 4600
    },
    {
      "epoch": 9.297725024727992,
      "grad_norm": 2.2130236625671387,
      "learning_rate": 0.0019170241970443342,
      "loss": 2.3347,
      "step": 4700
    },
    {
      "epoch": 9.495548961424332,
      "grad_norm": 1.6113771200180054,
      "learning_rate": 0.0019126651152015402,
      "loss": 2.419,
      "step": 4800
    },
    {
      "epoch": 9.693372898120673,
      "grad_norm": 1.4520199298858643,
      "learning_rate": 0.0019081996629078655,
      "loss": 2.4716,
      "step": 4900
    },
    {
      "epoch": 9.891196834817013,
      "grad_norm": 1.7119649648666382,
      "learning_rate": 0.0019036283606085054,
      "loss": 2.4397,
      "step": 5000
    },
    {
      "epoch": 10.089020771513352,
      "grad_norm": 1.7568297386169434,
      "learning_rate": 0.0018989517410853954,
      "loss": 2.3585,
      "step": 5100
    },
    {
      "epoch": 10.286844708209694,
      "grad_norm": 1.788895845413208,
      "learning_rate": 0.0018941703493951163,
      "loss": 2.2983,
      "step": 5200
    },
    {
      "epoch": 10.484668644906034,
      "grad_norm": 1.6706068515777588,
      "learning_rate": 0.0018892847428053693,
      "loss": 2.351,
      "step": 5300
    },
    {
      "epoch": 10.682492581602373,
      "grad_norm": 1.4277194738388062,
      "learning_rate": 0.0018842954907300237,
      "loss": 2.4211,
      "step": 5400
    },
    {
      "epoch": 10.880316518298715,
      "grad_norm": 1.7047756910324097,
      "learning_rate": 0.0018792031746627563,
      "loss": 2.4237,
      "step": 5500
    },
    {
      "epoch": 11.078140454995054,
      "grad_norm": 2.021911144256592,
      "learning_rate": 0.0018740083881092758,
      "loss": 2.3773,
      "step": 5600
    },
    {
      "epoch": 11.275964391691394,
      "grad_norm": 1.995078206062317,
      "learning_rate": 0.0018687117365181513,
      "loss": 2.264,
      "step": 5700
    },
    {
      "epoch": 11.473788328387736,
      "grad_norm": 1.7181724309921265,
      "learning_rate": 0.0018633138372102468,
      "loss": 2.3548,
      "step": 5800
    },
    {
      "epoch": 11.671612265084075,
      "grad_norm": 1.8769190311431885,
      "learning_rate": 0.0018578153193067745,
      "loss": 2.3418,
      "step": 5900
    },
    {
      "epoch": 11.869436201780415,
      "grad_norm": 1.5334117412567139,
      "learning_rate": 0.0018522168236559692,
      "loss": 2.4186,
      "step": 6000
    },
    {
      "epoch": 12.067260138476756,
      "grad_norm": 1.762642741203308,
      "learning_rate": 0.0018465190027584005,
      "loss": 2.318,
      "step": 6100
    },
    {
      "epoch": 12.265084075173096,
      "grad_norm": 1.7128247022628784,
      "learning_rate": 0.0018407225206909209,
      "loss": 2.2276,
      "step": 6200
    },
    {
      "epoch": 12.462908011869436,
      "grad_norm": 1.7851643562316895,
      "learning_rate": 0.0018348280530292712,
      "loss": 2.3089,
      "step": 6300
    },
    {
      "epoch": 12.660731948565777,
      "grad_norm": 1.7292343378067017,
      "learning_rate": 0.0018288362867693413,
      "loss": 2.3407,
      "step": 6400
    },
    {
      "epoch": 12.858555885262117,
      "grad_norm": 1.7729125022888184,
      "learning_rate": 0.0018227479202471014,
      "loss": 2.369,
      "step": 6500
    },
    {
      "epoch": 13.056379821958457,
      "grad_norm": 1.8102742433547974,
      "learning_rate": 0.001816563663057211,
      "loss": 2.2672,
      "step": 6600
    },
    {
      "epoch": 13.254203758654798,
      "grad_norm": 1.840323805809021,
      "learning_rate": 0.0018102842359703176,
      "loss": 2.1657,
      "step": 6700
    },
    {
      "epoch": 13.452027695351138,
      "grad_norm": 1.517435073852539,
      "learning_rate": 0.00180391037084905,
      "loss": 2.2748,
      "step": 6800
    },
    {
      "epoch": 13.649851632047477,
      "grad_norm": 2.100921869277954,
      "learning_rate": 0.0017974428105627207,
      "loss": 2.2905,
      "step": 6900
    },
    {
      "epoch": 13.847675568743817,
      "grad_norm": 1.7470345497131348,
      "learning_rate": 0.0017908823089007458,
      "loss": 2.33,
      "step": 7000
    },
    {
      "epoch": 14.045499505440159,
      "grad_norm": 1.6280616521835327,
      "learning_rate": 0.0017842296304847892,
      "loss": 2.3008,
      "step": 7100
    },
    {
      "epoch": 14.243323442136498,
      "grad_norm": 1.7211118936538696,
      "learning_rate": 0.0017774855506796495,
      "loss": 2.161,
      "step": 7200
    },
    {
      "epoch": 14.441147378832838,
      "grad_norm": 1.789689064025879,
      "learning_rate": 0.0017706508555028894,
      "loss": 2.2626,
      "step": 7300
    },
    {
      "epoch": 14.63897131552918,
      "grad_norm": 1.913696050643921,
      "learning_rate": 0.001763726341533227,
      "loss": 2.2197,
      "step": 7400
    },
    {
      "epoch": 14.836795252225519,
      "grad_norm": 1.8354309797286987,
      "learning_rate": 0.0017567128158176952,
      "loss": 2.318,
      "step": 7500
    },
    {
      "epoch": 15.034619188921859,
      "grad_norm": 1.840372085571289,
      "learning_rate": 0.0017496110957775808,
      "loss": 2.2489,
      "step": 7600
    },
    {
      "epoch": 15.2324431256182,
      "grad_norm": 1.787516713142395,
      "learning_rate": 0.0017424220091131536,
      "loss": 2.1276,
      "step": 7700
    },
    {
      "epoch": 15.43026706231454,
      "grad_norm": 1.862711787223816,
      "learning_rate": 0.0017351463937072004,
      "loss": 2.1628,
      "step": 7800
    },
    {
      "epoch": 15.62809099901088,
      "grad_norm": 1.7552751302719116,
      "learning_rate": 0.0017277850975273696,
      "loss": 2.2375,
      "step": 7900
    },
    {
      "epoch": 15.825914935707221,
      "grad_norm": 1.9924578666687012,
      "learning_rate": 0.00172033897852734,
      "loss": 2.2447,
      "step": 8000
    },
    {
      "epoch": 16.023738872403563,
      "grad_norm": 1.8519929647445679,
      "learning_rate": 0.0017128089045468293,
      "loss": 2.2328,
      "step": 8100
    },
    {
      "epoch": 16.2215628090999,
      "grad_norm": 1.9348610639572144,
      "learning_rate": 0.001705195753210446,
      "loss": 2.0776,
      "step": 8200
    },
    {
      "epoch": 16.419386745796242,
      "grad_norm": 1.5177891254425049,
      "learning_rate": 0.001697500411825403,
      "loss": 2.1186,
      "step": 8300
    },
    {
      "epoch": 16.617210682492583,
      "grad_norm": 1.8181427717208862,
      "learning_rate": 0.0016897237772781045,
      "loss": 2.175,
      "step": 8400
    },
    {
      "epoch": 16.81503461918892,
      "grad_norm": 2.0504062175750732,
      "learning_rate": 0.0016818667559296118,
      "loss": 2.2339,
      "step": 8500
    },
    {
      "epoch": 17.012858555885263,
      "grad_norm": 1.6926120519638062,
      "learning_rate": 0.0016739302635100108,
      "loss": 2.2184,
      "step": 8600
    },
    {
      "epoch": 17.2106824925816,
      "grad_norm": 1.7550945281982422,
      "learning_rate": 0.0016659152250116812,
      "loss": 2.0623,
      "step": 8700
    },
    {
      "epoch": 17.408506429277942,
      "grad_norm": 1.9956283569335938,
      "learning_rate": 0.0016578225745814909,
      "loss": 2.0693,
      "step": 8800
    },
    {
      "epoch": 17.606330365974284,
      "grad_norm": 1.909571647644043,
      "learning_rate": 0.0016496532554119213,
      "loss": 2.1595,
      "step": 8900
    },
    {
      "epoch": 17.80415430267062,
      "grad_norm": 1.9219504594802856,
      "learning_rate": 0.00164140821963114,
      "loss": 2.1695,
      "step": 9000
    },
    {
      "epoch": 18.001978239366963,
      "grad_norm": 1.7251026630401611,
      "learning_rate": 0.00163308842819203,
      "loss": 2.1687,
      "step": 9100
    },
    {
      "epoch": 18.199802176063304,
      "grad_norm": 2.0570778846740723,
      "learning_rate": 0.0016246948507601913,
      "loss": 1.9721,
      "step": 9200
    },
    {
      "epoch": 18.397626112759642,
      "grad_norm": 1.8878275156021118,
      "learning_rate": 0.0016162284656009273,
      "loss": 2.0471,
      "step": 9300
    },
    {
      "epoch": 18.595450049455984,
      "grad_norm": 1.9205576181411743,
      "learning_rate": 0.001607690259465229,
      "loss": 2.1127,
      "step": 9400
    },
    {
      "epoch": 18.793273986152325,
      "grad_norm": 1.8654621839523315,
      "learning_rate": 0.0015990812274747693,
      "loss": 2.1183,
      "step": 9500
    },
    {
      "epoch": 18.991097922848663,
      "grad_norm": 1.75041663646698,
      "learning_rate": 0.0015904023730059227,
      "loss": 2.1456,
      "step": 9600
    },
    {
      "epoch": 19.188921859545005,
      "grad_norm": 1.7423347234725952,
      "learning_rate": 0.0015816547075728226,
      "loss": 1.971,
      "step": 9700
    },
    {
      "epoch": 19.386745796241346,
      "grad_norm": 1.9034976959228516,
      "learning_rate": 0.0015728392507094698,
      "loss": 2.003,
      "step": 9800
    },
    {
      "epoch": 19.584569732937684,
      "grad_norm": 1.9860583543777466,
      "learning_rate": 0.0015639570298509064,
      "loss": 2.0255,
      "step": 9900
    },
    {
      "epoch": 19.782393669634025,
      "grad_norm": 1.6944961547851562,
      "learning_rate": 0.00155500908021347,
      "loss": 2.0828,
      "step": 10000
    },
    {
      "epoch": 19.980217606330367,
      "grad_norm": 2.1065680980682373,
      "learning_rate": 0.0015459964446741382,
      "loss": 2.1153,
      "step": 10100
    },
    {
      "epoch": 20.178041543026705,
      "grad_norm": 2.0042524337768555,
      "learning_rate": 0.0015369201736489839,
      "loss": 1.9475,
      "step": 10200
    },
    {
      "epoch": 20.375865479723046,
      "grad_norm": 1.7623581886291504,
      "learning_rate": 0.0015277813249707486,
      "loss": 1.9525,
      "step": 10300
    },
    {
      "epoch": 20.573689416419388,
      "grad_norm": 1.9074281454086304,
      "learning_rate": 0.0015185809637655548,
      "loss": 1.987,
      "step": 10400
    },
    {
      "epoch": 20.771513353115726,
      "grad_norm": 1.7943137884140015,
      "learning_rate": 0.001509320162328763,
      "loss": 2.0019,
      "step": 10500
    },
    {
      "epoch": 20.969337289812067,
      "grad_norm": 2.0741937160491943,
      "learning_rate": 0.0015,
      "loss": 2.0563,
      "step": 10600
    },
    {
      "epoch": 21.16716122650841,
      "grad_norm": 1.6963410377502441,
      "learning_rate": 0.0014906215630373606,
      "loss": 1.8835,
      "step": 10700
    },
    {
      "epoch": 21.364985163204746,
      "grad_norm": 1.8931708335876465,
      "learning_rate": 0.001481185944490805,
      "loss": 1.884,
      "step": 10800
    },
    {
      "epoch": 21.562809099901088,
      "grad_norm": 1.913797378540039,
      "learning_rate": 0.0014716942440747662,
      "loss": 1.9592,
      "step": 10900
    },
    {
      "epoch": 21.76063303659743,
      "grad_norm": 1.6734392642974854,
      "learning_rate": 0.001462147568039977,
      "loss": 1.9849,
      "step": 11000
    },
    {
      "epoch": 21.958456973293767,
      "grad_norm": 1.883903980255127,
      "learning_rate": 0.0014525470290445391,
      "loss": 2.0197,
      "step": 11100
    },
    {
      "epoch": 22.15628090999011,
      "grad_norm": 1.9503158330917358,
      "learning_rate": 0.0014428937460242417,
      "loss": 1.8056,
      "step": 11200
    },
    {
      "epoch": 22.35410484668645,
      "grad_norm": 1.8135210275650024,
      "learning_rate": 0.0014331888440621532,
      "loss": 1.8749,
      "step": 11300
    },
    {
      "epoch": 22.551928783382788,
      "grad_norm": 1.9343315362930298,
      "learning_rate": 0.0014234334542574906,
      "loss": 1.8779,
      "step": 11400
    },
    {
      "epoch": 22.74975272007913,
      "grad_norm": 2.1252267360687256,
      "learning_rate": 0.0014136287135937916,
      "loss": 1.9338,
      "step": 11500
    },
    {
      "epoch": 22.94757665677547,
      "grad_norm": 2.0103166103363037,
      "learning_rate": 0.0014037757648064017,
      "loss": 1.9707,
      "step": 11600
    },
    {
      "epoch": 23.14540059347181,
      "grad_norm": 2.149522542953491,
      "learning_rate": 0.0013938757562492873,
      "loss": 1.7779,
      "step": 11700
    },
    {
      "epoch": 23.34322453016815,
      "grad_norm": 2.1340994834899902,
      "learning_rate": 0.0013839298417611962,
      "loss": 1.8248,
      "step": 11800
    },
    {
      "epoch": 23.541048466864492,
      "grad_norm": 1.9957151412963867,
      "learning_rate": 0.0013739391805311794,
      "loss": 1.8016,
      "step": 11900
    },
    {
      "epoch": 23.73887240356083,
      "grad_norm": 2.0001652240753174,
      "learning_rate": 0.0013639049369634877,
      "loss": 1.8833,
      "step": 12000
    },
    {
      "epoch": 23.93669634025717,
      "grad_norm": 2.056978940963745,
      "learning_rate": 0.0013538282805418609,
      "loss": 1.9129,
      "step": 12100
    },
    {
      "epoch": 24.134520276953513,
      "grad_norm": 2.0528552532196045,
      "learning_rate": 0.0013437103856932264,
      "loss": 1.7479,
      "step": 12200
    },
    {
      "epoch": 24.33234421364985,
      "grad_norm": 2.0190505981445312,
      "learning_rate": 0.0013335524316508208,
      "loss": 1.7152,
      "step": 12300
    },
    {
      "epoch": 24.530168150346192,
      "grad_norm": 2.1085124015808105,
      "learning_rate": 0.0013233556023167486,
      "loss": 1.8376,
      "step": 12400
    },
    {
      "epoch": 24.727992087042534,
      "grad_norm": 1.8438692092895508,
      "learning_rate": 0.0013131210861240027,
      "loss": 1.8179,
      "step": 12500
    },
    {
      "epoch": 24.92581602373887,
      "grad_norm": 1.8953475952148438,
      "learning_rate": 0.0013028500758979506,
      "loss": 1.8296,
      "step": 12600
    },
    {
      "epoch": 25.123639960435213,
      "grad_norm": 2.0940098762512207,
      "learning_rate": 0.0012925437687173144,
      "loss": 1.7174,
      "step": 12700
    },
    {
      "epoch": 25.321463897131554,
      "grad_norm": 2.296750068664551,
      "learning_rate": 0.0012822033657746478,
      "loss": 1.6598,
      "step": 12800
    },
    {
      "epoch": 25.519287833827892,
      "grad_norm": 2.092942237854004,
      "learning_rate": 0.001271830072236343,
      "loss": 1.7358,
      "step": 12900
    },
    {
      "epoch": 25.717111770524234,
      "grad_norm": 2.1927082538604736,
      "learning_rate": 0.0012614250971021658,
      "loss": 1.8037,
      "step": 13000
    },
    {
      "epoch": 25.914935707220575,
      "grad_norm": 2.028815984725952,
      "learning_rate": 0.0012509896530643488,
      "loss": 1.7608,
      "step": 13100
    },
    {
      "epoch": 26.112759643916913,
      "grad_norm": 1.929516077041626,
      "learning_rate": 0.0012405249563662538,
      "loss": 1.6745,
      "step": 13200
    },
    {
      "epoch": 26.310583580613255,
      "grad_norm": 2.040217399597168,
      "learning_rate": 0.0012300322266606176,
      "loss": 1.6577,
      "step": 13300
    },
    {
      "epoch": 26.508407517309596,
      "grad_norm": 2.076997756958008,
      "learning_rate": 0.001219512686867405,
      "loss": 1.6608,
      "step": 13400
    },
    {
      "epoch": 26.706231454005934,
      "grad_norm": 2.0372560024261475,
      "learning_rate": 0.0012089675630312753,
      "loss": 1.6879,
      "step": 13500
    },
    {
      "epoch": 26.904055390702275,
      "grad_norm": 1.836409568786621,
      "learning_rate": 0.0011983980841786899,
      "loss": 1.6881,
      "step": 13600
    },
    {
      "epoch": 27.101879327398617,
      "grad_norm": 2.141629934310913,
      "learning_rate": 0.0011878054821746703,
      "loss": 1.638,
      "step": 13700
    },
    {
      "epoch": 27.299703264094955,
      "grad_norm": 2.1385319232940674,
      "learning_rate": 0.0011771909915792229,
      "loss": 1.5534,
      "step": 13800
    },
    {
      "epoch": 27.497527200791296,
      "grad_norm": 2.2524759769439697,
      "learning_rate": 0.0011665558495034545,
      "loss": 1.5912,
      "step": 13900
    },
    {
      "epoch": 27.695351137487634,
      "grad_norm": 1.8820711374282837,
      "learning_rate": 0.0011559012954653865,
      "loss": 1.6315,
      "step": 14000
    },
    {
      "epoch": 27.893175074183976,
      "grad_norm": 2.2426018714904785,
      "learning_rate": 0.0011452285712454905,
      "loss": 1.6756,
      "step": 14100
    },
    {
      "epoch": 28.090999010880317,
      "grad_norm": 2.2863082885742188,
      "learning_rate": 0.0011345389207419588,
      "loss": 1.5812,
      "step": 14200
    },
    {
      "epoch": 28.288822947576655,
      "grad_norm": 2.3093032836914062,
      "learning_rate": 0.0011238335898257304,
      "loss": 1.5026,
      "step": 14300
    },
    {
      "epoch": 28.486646884272997,
      "grad_norm": 2.1801974773406982,
      "learning_rate": 0.0011131138261952845,
      "loss": 1.541,
      "step": 14400
    },
    {
      "epoch": 28.684470820969338,
      "grad_norm": 2.134673595428467,
      "learning_rate": 0.0011023808792312226,
      "loss": 1.5898,
      "step": 14500
    },
    {
      "epoch": 28.882294757665676,
      "grad_norm": 2.0571701526641846,
      "learning_rate": 0.0010916359998506548,
      "loss": 1.5908,
      "step": 14600
    },
    {
      "epoch": 29.080118694362017,
      "grad_norm": 2.3080058097839355,
      "learning_rate": 0.0010808804403614042,
      "loss": 1.5322,
      "step": 14700
    },
    {
      "epoch": 29.27794263105836,
      "grad_norm": 2.1384010314941406,
      "learning_rate": 0.0010701154543160541,
      "loss": 1.4387,
      "step": 14800
    },
    {
      "epoch": 29.475766567754697,
      "grad_norm": 2.3306639194488525,
      "learning_rate": 0.0010593422963658453,
      "loss": 1.4774,
      "step": 14900
    },
    {
      "epoch": 29.673590504451038,
      "grad_norm": 2.045041561126709,
      "learning_rate": 0.0010485622221144484,
      "loss": 1.5162,
      "step": 15000
    },
    {
      "epoch": 29.87141444114738,
      "grad_norm": 2.332676410675049,
      "learning_rate": 0.0010377764879716234,
      "loss": 1.5462,
      "step": 15100
    },
    {
      "epoch": 30.069238377843718,
      "grad_norm": 1.9611519575119019,
      "learning_rate": 0.0010269863510067871,
      "loss": 1.4831,
      "step": 15200
    },
    {
      "epoch": 30.26706231454006,
      "grad_norm": 2.141835927963257,
      "learning_rate": 0.0010161930688025015,
      "loss": 1.3574,
      "step": 15300
    },
    {
      "epoch": 30.4648862512364,
      "grad_norm": 2.2579574584960938,
      "learning_rate": 0.0010053978993079045,
      "loss": 1.4432,
      "step": 15400
    },
    {
      "epoch": 30.66271018793274,
      "grad_norm": 2.042375326156616,
      "learning_rate": 0.0009946021006920957,
      "loss": 1.4531,
      "step": 15500
    },
    {
      "epoch": 30.86053412462908,
      "grad_norm": 2.3491597175598145,
      "learning_rate": 0.0009838069311974985,
      "loss": 1.4497,
      "step": 15600
    },
    {
      "epoch": 31.05835806132542,
      "grad_norm": 2.0492799282073975,
      "learning_rate": 0.0009730136489932132,
      "loss": 1.4104,
      "step": 15700
    },
    {
      "epoch": 31.25618199802176,
      "grad_norm": 2.4593396186828613,
      "learning_rate": 0.0009622235120283768,
      "loss": 1.2777,
      "step": 15800
    },
    {
      "epoch": 31.4540059347181,
      "grad_norm": 2.0360829830169678,
      "learning_rate": 0.000951437777885552,
      "loss": 1.3474,
      "step": 15900
    },
    {
      "epoch": 31.651829871414442,
      "grad_norm": 2.0964555740356445,
      "learning_rate": 0.0009406577036341548,
      "loss": 1.4025,
      "step": 16000
    },
    {
      "epoch": 31.84965380811078,
      "grad_norm": 2.406019926071167,
      "learning_rate": 0.0009298845456839459,
      "loss": 1.4256,
      "step": 16100
    },
    {
      "epoch": 32.047477744807125,
      "grad_norm": 2.3522233963012695,
      "learning_rate": 0.0009191195596385959,
      "loss": 1.3926,
      "step": 16200
    },
    {
      "epoch": 32.24530168150346,
      "grad_norm": 2.110896110534668,
      "learning_rate": 0.0009083640001493454,
      "loss": 1.2064,
      "step": 16300
    },
    {
      "epoch": 32.4431256181998,
      "grad_norm": 2.212249517440796,
      "learning_rate": 0.0008976191207687775,
      "loss": 1.2725,
      "step": 16400
    },
    {
      "epoch": 32.640949554896146,
      "grad_norm": 2.1431198120117188,
      "learning_rate": 0.0008868861738047158,
      "loss": 1.3331,
      "step": 16500
    },
    {
      "epoch": 32.838773491592484,
      "grad_norm": 2.148728370666504,
      "learning_rate": 0.00087616641017427,
      "loss": 1.3597,
      "step": 16600
    },
    {
      "epoch": 33.03659742828882,
      "grad_norm": 2.4619946479797363,
      "learning_rate": 0.0008654610792580415,
      "loss": 1.3506,
      "step": 16700
    },
    {
      "epoch": 33.23442136498517,
      "grad_norm": 2.297394037246704,
      "learning_rate": 0.00085477142875451,
      "loss": 1.162,
      "step": 16800
    },
    {
      "epoch": 33.432245301681505,
      "grad_norm": 2.1372103691101074,
      "learning_rate": 0.0008440987045346134,
      "loss": 1.2197,
      "step": 16900
    },
    {
      "epoch": 33.63006923837784,
      "grad_norm": 2.5905022621154785,
      "learning_rate": 0.0008334441504965456,
      "loss": 1.2446,
      "step": 17000
    },
    {
      "epoch": 33.82789317507418,
      "grad_norm": 2.2432897090911865,
      "learning_rate": 0.0008228090084207773,
      "loss": 1.2685,
      "step": 17100
    },
    {
      "epoch": 34.025717111770525,
      "grad_norm": 2.3998472690582275,
      "learning_rate": 0.00081219451782533,
      "loss": 1.3188,
      "step": 17200
    },
    {
      "epoch": 34.22354104846686,
      "grad_norm": 2.2663626670837402,
      "learning_rate": 0.0008016019158213102,
      "loss": 1.0892,
      "step": 17300
    },
    {
      "epoch": 34.4213649851632,
      "grad_norm": 2.413686752319336,
      "learning_rate": 0.000791032436968725,
      "loss": 1.155,
      "step": 17400
    },
    {
      "epoch": 34.619188921859546,
      "grad_norm": 2.31451153755188,
      "learning_rate": 0.0007804873131325953,
      "loss": 1.1901,
      "step": 17500
    },
    {
      "epoch": 34.817012858555884,
      "grad_norm": 2.1165082454681396,
      "learning_rate": 0.0007699677733393826,
      "loss": 1.2199,
      "step": 17600
    },
    {
      "epoch": 35.01483679525222,
      "grad_norm": 2.016362190246582,
      "learning_rate": 0.0007594750436337467,
      "loss": 1.2166,
      "step": 17700
    },
    {
      "epoch": 35.21266073194857,
      "grad_norm": 2.346348524093628,
      "learning_rate": 0.0007490103469356513,
      "loss": 1.0469,
      "step": 17800
    },
    {
      "epoch": 35.410484668644905,
      "grad_norm": 2.463761329650879,
      "learning_rate": 0.0007385749028978346,
      "loss": 1.0941,
      "step": 17900
    },
    {
      "epoch": 35.60830860534124,
      "grad_norm": 2.6038095951080322,
      "learning_rate": 0.0007281699277636571,
      "loss": 1.1271,
      "step": 18000
    },
    {
      "epoch": 35.80613254203759,
      "grad_norm": 2.206606864929199,
      "learning_rate": 0.0007177966342253524,
      "loss": 1.1393,
      "step": 18100
    },
    {
      "epoch": 36.003956478733926,
      "grad_norm": 1.9918839931488037,
      "learning_rate": 0.000707456231282686,
      "loss": 1.1769,
      "step": 18200
    },
    {
      "epoch": 36.201780415430264,
      "grad_norm": 2.5225119590759277,
      "learning_rate": 0.0006971499241020494,
      "loss": 0.9861,
      "step": 18300
    },
    {
      "epoch": 36.39960435212661,
      "grad_norm": 2.4943225383758545,
      "learning_rate": 0.0006868789138759977,
      "loss": 1.0195,
      "step": 18400
    },
    {
      "epoch": 36.59742828882295,
      "grad_norm": 2.6946022510528564,
      "learning_rate": 0.0006766443976832517,
      "loss": 1.0746,
      "step": 18500
    },
    {
      "epoch": 36.795252225519285,
      "grad_norm": 2.500440835952759,
      "learning_rate": 0.0006664475683491796,
      "loss": 1.1035,
      "step": 18600
    },
    {
      "epoch": 36.99307616221563,
      "grad_norm": 2.1558949947357178,
      "learning_rate": 0.0006562896143067733,
      "loss": 1.0862,
      "step": 18700
    },
    {
      "epoch": 37.19090009891197,
      "grad_norm": 2.5494956970214844,
      "learning_rate": 0.0006461717194581394,
      "loss": 0.9635,
      "step": 18800
    },
    {
      "epoch": 37.388724035608305,
      "grad_norm": 2.679243803024292,
      "learning_rate": 0.0006360950630365126,
      "loss": 0.9594,
      "step": 18900
    },
    {
      "epoch": 37.58654797230465,
      "grad_norm": 2.371471881866455,
      "learning_rate": 0.0006260608194688206,
      "loss": 0.9993,
      "step": 19000
    },
    {
      "epoch": 37.78437190900099,
      "grad_norm": 2.434765100479126,
      "learning_rate": 0.0006160701582388038,
      "loss": 1.0229,
      "step": 19100
    },
    {
      "epoch": 37.982195845697326,
      "grad_norm": 2.418194532394409,
      "learning_rate": 0.0006061242437507131,
      "loss": 1.0192,
      "step": 19200
    },
    {
      "epoch": 38.18001978239367,
      "grad_norm": 2.428098678588867,
      "learning_rate": 0.0005962242351935984,
      "loss": 0.8927,
      "step": 19300
    },
    {
      "epoch": 38.37784371909001,
      "grad_norm": 2.490022659301758,
      "learning_rate": 0.0005863712864062089,
      "loss": 0.9144,
      "step": 19400
    },
    {
      "epoch": 38.57566765578635,
      "grad_norm": 2.240866184234619,
      "learning_rate": 0.0005765665457425102,
      "loss": 0.9372,
      "step": 19500
    },
    {
      "epoch": 38.77349159248269,
      "grad_norm": 2.4496891498565674,
      "learning_rate": 0.000566811155937847,
      "loss": 0.9675,
      "step": 19600
    },
    {
      "epoch": 38.97131552917903,
      "grad_norm": 2.2578465938568115,
      "learning_rate": 0.0005571062539757581,
      "loss": 0.9706,
      "step": 19700
    },
    {
      "epoch": 39.16913946587537,
      "grad_norm": 2.393901824951172,
      "learning_rate": 0.0005474529709554612,
      "loss": 0.8364,
      "step": 19800
    },
    {
      "epoch": 39.36696340257171,
      "grad_norm": 2.5250625610351562,
      "learning_rate": 0.0005378524319600231,
      "loss": 0.8535,
      "step": 19900
    },
    {
      "epoch": 39.56478733926805,
      "grad_norm": 3.0035462379455566,
      "learning_rate": 0.0005283057559252342,
      "loss": 0.8973,
      "step": 20000
    },
    {
      "epoch": 39.76261127596439,
      "grad_norm": 2.932170867919922,
      "learning_rate": 0.0005188140555091949,
      "loss": 0.8802,
      "step": 20100
    },
    {
      "epoch": 39.960435212660734,
      "grad_norm": 2.732318639755249,
      "learning_rate": 0.0005093784369626397,
      "loss": 0.9202,
      "step": 20200
    },
    {
      "epoch": 40.15825914935707,
      "grad_norm": 2.1872189044952393,
      "learning_rate": 0.0005000000000000002,
      "loss": 0.8039,
      "step": 20300
    },
    {
      "epoch": 40.35608308605341,
      "grad_norm": 2.8175759315490723,
      "learning_rate": 0.0004906798376712373,
      "loss": 0.8082,
      "step": 20400
    },
    {
      "epoch": 40.553907022749755,
      "grad_norm": 2.5111172199249268,
      "learning_rate": 0.00048141903623444537,
      "loss": 0.8153,
      "step": 20500
    },
    {
      "epoch": 40.75173095944609,
      "grad_norm": 2.2794506549835205,
      "learning_rate": 0.0004722186750292511,
      "loss": 0.8478,
      "step": 20600
    },
    {
      "epoch": 40.94955489614243,
      "grad_norm": 2.551337480545044,
      "learning_rate": 0.0004630798263510162,
      "loss": 0.8565,
      "step": 20700
    },
    {
      "epoch": 41.147378832838776,
      "grad_norm": 2.456972360610962,
      "learning_rate": 0.0004540035553258619,
      "loss": 0.7601,
      "step": 20800
    },
    {
      "epoch": 41.34520276953511,
      "grad_norm": 2.282243490219116,
      "learning_rate": 0.0004449909197865303,
      "loss": 0.7499,
      "step": 20900
    },
    {
      "epoch": 41.54302670623145,
      "grad_norm": 2.0821707248687744,
      "learning_rate": 0.0004360429701490934,
      "loss": 0.7748,
      "step": 21000
    },
    {
      "epoch": 41.740850642927796,
      "grad_norm": 2.1007697582244873,
      "learning_rate": 0.0004271607492905303,
      "loss": 0.7883,
      "step": 21100
    },
    {
      "epoch": 41.938674579624134,
      "grad_norm": 2.491163969039917,
      "learning_rate": 0.00041834529242717756,
      "loss": 0.7952,
      "step": 21200
    },
    {
      "epoch": 42.13649851632047,
      "grad_norm": 2.2475414276123047,
      "learning_rate": 0.00040959762699407763,
      "loss": 0.7148,
      "step": 21300
    },
    {
      "epoch": 42.33432245301682,
      "grad_norm": 2.215935468673706,
      "learning_rate": 0.0004009187725252309,
      "loss": 0.7027,
      "step": 21400
    },
    {
      "epoch": 42.532146389713155,
      "grad_norm": 2.485879898071289,
      "learning_rate": 0.00039230974053477085,
      "loss": 0.7179,
      "step": 21500
    },
    {
      "epoch": 42.72997032640949,
      "grad_norm": 2.3877782821655273,
      "learning_rate": 0.00038377153439907266,
      "loss": 0.7474,
      "step": 21600
    },
    {
      "epoch": 42.92779426310584,
      "grad_norm": 2.456117630004883,
      "learning_rate": 0.00037530514923980886,
      "loss": 0.759,
      "step": 21700
    },
    {
      "epoch": 43.125618199802176,
      "grad_norm": 2.4997050762176514,
      "learning_rate": 0.0003669115718079702,
      "loss": 0.6869,
      "step": 21800
    },
    {
      "epoch": 43.323442136498514,
      "grad_norm": 2.283956527709961,
      "learning_rate": 0.0003585917803688603,
      "loss": 0.6427,
      "step": 21900
    },
    {
      "epoch": 43.52126607319486,
      "grad_norm": 1.9921088218688965,
      "learning_rate": 0.00035034674458807893,
      "loss": 0.6806,
      "step": 22000
    },
    {
      "epoch": 43.7190900098912,
      "grad_norm": 2.284411907196045,
      "learning_rate": 0.0003421774254185096,
      "loss": 0.6896,
      "step": 22100
    },
    {
      "epoch": 43.916913946587535,
      "grad_norm": 2.032017469406128,
      "learning_rate": 0.00033408477498831913,
      "loss": 0.7026,
      "step": 22200
    },
    {
      "epoch": 44.11473788328388,
      "grad_norm": 2.226177930831909,
      "learning_rate": 0.0003260697364899892,
      "loss": 0.6487,
      "step": 22300
    },
    {
      "epoch": 44.31256181998022,
      "grad_norm": 2.465562343597412,
      "learning_rate": 0.00031813324407038826,
      "loss": 0.6141,
      "step": 22400
    },
    {
      "epoch": 44.510385756676556,
      "grad_norm": 2.9031460285186768,
      "learning_rate": 0.00031027622272189573,
      "loss": 0.6418,
      "step": 22500
    },
    {
      "epoch": 44.7082096933729,
      "grad_norm": 2.3090295791625977,
      "learning_rate": 0.0003024995881745972,
      "loss": 0.6449,
      "step": 22600
    },
    {
      "epoch": 44.90603363006924,
      "grad_norm": 2.2223401069641113,
      "learning_rate": 0.0002948042467895544,
      "loss": 0.6578,
      "step": 22700
    },
    {
      "epoch": 45.103857566765576,
      "grad_norm": 2.0777714252471924,
      "learning_rate": 0.00028719109545317104,
      "loss": 0.6224,
      "step": 22800
    },
    {
      "epoch": 45.30168150346192,
      "grad_norm": 2.5618162155151367,
      "learning_rate": 0.0002796610214726599,
      "loss": 0.5843,
      "step": 22900
    },
    {
      "epoch": 45.49950544015826,
      "grad_norm": 2.414086103439331,
      "learning_rate": 0.0002722149024726307,
      "loss": 0.6007,
      "step": 23000
    },
    {
      "epoch": 45.6973293768546,
      "grad_norm": 2.2268996238708496,
      "learning_rate": 0.0002648536062927999,
      "loss": 0.6072,
      "step": 23100
    },
    {
      "epoch": 45.89515331355094,
      "grad_norm": 2.8498668670654297,
      "learning_rate": 0.0002575779908868465,
      "loss": 0.606,
      "step": 23200
    },
    {
      "epoch": 46.09297725024728,
      "grad_norm": 2.6216135025024414,
      "learning_rate": 0.00025038890422241953,
      "loss": 0.5722,
      "step": 23300
    },
    {
      "epoch": 46.29080118694362,
      "grad_norm": 1.9540680646896362,
      "learning_rate": 0.00024328718418230468,
      "loss": 0.5409,
      "step": 23400
    },
    {
      "epoch": 46.48862512363996,
      "grad_norm": 2.129328966140747,
      "learning_rate": 0.00023627365846677306,
      "loss": 0.5601,
      "step": 23500
    },
    {
      "epoch": 46.6864490603363,
      "grad_norm": 2.4610815048217773,
      "learning_rate": 0.00022934914449711087,
      "loss": 0.5725,
      "step": 23600
    },
    {
      "epoch": 46.88427299703264,
      "grad_norm": 2.401482343673706,
      "learning_rate": 0.0002225144493203509,
      "loss": 0.5723,
      "step": 23700
    },
    {
      "epoch": 47.082096933728984,
      "grad_norm": 2.2367210388183594,
      "learning_rate": 0.00021577036951521089,
      "loss": 0.5508,
      "step": 23800
    },
    {
      "epoch": 47.27992087042532,
      "grad_norm": 2.05940580368042,
      "learning_rate": 0.0002091176910992545,
      "loss": 0.5072,
      "step": 23900
    },
    {
      "epoch": 47.47774480712166,
      "grad_norm": 2.131868600845337,
      "learning_rate": 0.0002025571894372794,
      "loss": 0.5381,
      "step": 24000
    },
    {
      "epoch": 47.675568743818005,
      "grad_norm": 2.2878973484039307,
      "learning_rate": 0.00019608962915094996,
      "loss": 0.5416,
      "step": 24100
    },
    {
      "epoch": 47.87339268051434,
      "grad_norm": 2.0183324813842773,
      "learning_rate": 0.00018971576402968248,
      "loss": 0.5435,
      "step": 24200
    },
    {
      "epoch": 48.07121661721068,
      "grad_norm": 1.841946005821228,
      "learning_rate": 0.00018343633694278895,
      "loss": 0.5107,
      "step": 24300
    },
    {
      "epoch": 48.269040553907026,
      "grad_norm": 2.212181568145752,
      "learning_rate": 0.00017725207975289881,
      "loss": 0.4859,
      "step": 24400
    },
    {
      "epoch": 48.46686449060336,
      "grad_norm": 2.1341381072998047,
      "learning_rate": 0.00017116371323065883,
      "loss": 0.4934,
      "step": 24500
    },
    {
      "epoch": 48.6646884272997,
      "grad_norm": 2.383610486984253,
      "learning_rate": 0.00016517194697072903,
      "loss": 0.5099,
      "step": 24600
    },
    {
      "epoch": 48.862512363996046,
      "grad_norm": 2.091445207595825,
      "learning_rate": 0.0001592774793090792,
      "loss": 0.5225,
      "step": 24700
    },
    {
      "epoch": 49.060336300692384,
      "grad_norm": 2.2177491188049316,
      "learning_rate": 0.00015348099724159982,
      "loss": 0.4932,
      "step": 24800
    },
    {
      "epoch": 49.25816023738872,
      "grad_norm": 1.8907312154769897,
      "learning_rate": 0.0001477831763440308,
      "loss": 0.4652,
      "step": 24900
    },
    {
      "epoch": 49.45598417408507,
      "grad_norm": 2.144429922103882,
      "learning_rate": 0.00014218468069322578,
      "loss": 0.473,
      "step": 25000
    },
    {
      "epoch": 49.653808110781405,
      "grad_norm": 2.1019511222839355,
      "learning_rate": 0.00013668616278975342,
      "loss": 0.4702,
      "step": 25100
    },
    {
      "epoch": 49.85163204747774,
      "grad_norm": 2.052175760269165,
      "learning_rate": 0.00013128826348184885,
      "loss": 0.4815,
      "step": 25200
    },
    {
      "epoch": 50.04945598417409,
      "grad_norm": 2.162271738052368,
      "learning_rate": 0.00012599161189072428,
      "loss": 0.4818,
      "step": 25300
    },
    {
      "epoch": 50.247279920870426,
      "grad_norm": 2.315767288208008,
      "learning_rate": 0.00012079682533724379,
      "loss": 0.4426,
      "step": 25400
    },
    {
      "epoch": 50.445103857566764,
      "grad_norm": 1.803246259689331,
      "learning_rate": 0.00011570450926997656,
      "loss": 0.4473,
      "step": 25500
    },
    {
      "epoch": 50.64292779426311,
      "grad_norm": 2.3311853408813477,
      "learning_rate": 0.00011071525719463094,
      "loss": 0.4668,
      "step": 25600
    },
    {
      "epoch": 50.84075173095945,
      "grad_norm": 2.028862714767456,
      "learning_rate": 0.00010582965060488359,
      "loss": 0.4611,
      "step": 25700
    },
    {
      "epoch": 51.038575667655785,
      "grad_norm": 1.747746467590332,
      "learning_rate": 0.00010104825891460479,
      "loss": 0.446,
      "step": 25800
    },
    {
      "epoch": 51.23639960435213,
      "grad_norm": 2.28305721282959,
      "learning_rate": 9.637163939149484e-05,
      "loss": 0.4275,
      "step": 25900
    },
    {
      "epoch": 51.43422354104847,
      "grad_norm": 2.0028929710388184,
      "learning_rate": 9.180033709213453e-05,
      "loss": 0.4323,
      "step": 26000
    },
    {
      "epoch": 51.632047477744806,
      "grad_norm": 2.2836782932281494,
      "learning_rate": 8.733488479845996e-05,
      "loss": 0.4385,
      "step": 26100
    },
    {
      "epoch": 51.82987141444115,
      "grad_norm": 1.9806368350982666,
      "learning_rate": 8.297580295566575e-05,
      "loss": 0.4359,
      "step": 26200
    },
    {
      "epoch": 52.02769535113749,
      "grad_norm": 1.5683083534240723,
      "learning_rate": 7.872359961154906e-05,
      "loss": 0.4372,
      "step": 26300
    },
    {
      "epoch": 52.225519287833826,
      "grad_norm": 1.9526455402374268,
      "learning_rate": 7.457877035729587e-05,
      "loss": 0.4001,
      "step": 26400
    },
    {
      "epoch": 52.42334322453017,
      "grad_norm": 2.170952558517456,
      "learning_rate": 7.054179826972074e-05,
      "loss": 0.4163,
      "step": 26500
    },
    {
      "epoch": 52.62116716122651,
      "grad_norm": 1.7725653648376465,
      "learning_rate": 6.661315385496424e-05,
      "loss": 0.4263,
      "step": 26600
    },
    {
      "epoch": 52.81899109792285,
      "grad_norm": 1.927458643913269,
      "learning_rate": 6.279329499365649e-05,
      "loss": 0.4185,
      "step": 26700
    },
    {
      "epoch": 53.01681503461919,
      "grad_norm": 1.864128589630127,
      "learning_rate": 5.908266688755049e-05,
      "loss": 0.4265,
      "step": 26800
    },
    {
      "epoch": 53.21463897131553,
      "grad_norm": 2.1848411560058594,
      "learning_rate": 5.5481702007634694e-05,
      "loss": 0.4008,
      "step": 26900
    },
    {
      "epoch": 53.41246290801187,
      "grad_norm": 1.8295989036560059,
      "learning_rate": 5.199082004372957e-05,
      "loss": 0.401,
      "step": 27000
    },
    {
      "epoch": 53.61028684470821,
      "grad_norm": 1.8910940885543823,
      "learning_rate": 4.861042785557146e-05,
      "loss": 0.4032,
      "step": 27100
    },
    {
      "epoch": 53.80811078140455,
      "grad_norm": 2.376465082168579,
      "learning_rate": 4.5340919425394756e-05,
      "loss": 0.4082,
      "step": 27200
    },
    {
      "epoch": 54.00593471810089,
      "grad_norm": 1.9435615539550781,
      "learning_rate": 4.218267581201296e-05,
      "loss": 0.4101,
      "step": 27300
    },
    {
      "epoch": 54.203758654797234,
      "grad_norm": 1.8882975578308105,
      "learning_rate": 3.913606510640644e-05,
      "loss": 0.3967,
      "step": 27400
    },
    {
      "epoch": 54.40158259149357,
      "grad_norm": 1.7718919515609741,
      "learning_rate": 3.620144238882206e-05,
      "loss": 0.3861,
      "step": 27500
    },
    {
      "epoch": 54.59940652818991,
      "grad_norm": 1.649854302406311,
      "learning_rate": 3.337914968738887e-05,
      "loss": 0.3907,
      "step": 27600
    },
    {
      "epoch": 54.797230464886255,
      "grad_norm": 2.256138563156128,
      "learning_rate": 3.06695159382544e-05,
      "loss": 0.3888,
      "step": 27700
    },
    {
      "epoch": 54.99505440158259,
      "grad_norm": 2.1119978427886963,
      "learning_rate": 2.8072856947248037e-05,
      "loss": 0.4074,
      "step": 27800
    },
    {
      "epoch": 55.19287833827893,
      "grad_norm": 1.9851913452148438,
      "learning_rate": 2.5589475353073986e-05,
      "loss": 0.3868,
      "step": 27900
    },
    {
      "epoch": 55.39070227497527,
      "grad_norm": 2.3545823097229004,
      "learning_rate": 2.3219660592038284e-05,
      "loss": 0.3806,
      "step": 28000
    },
    {
      "epoch": 55.58852621167161,
      "grad_norm": 2.0634751319885254,
      "learning_rate": 2.096368886431632e-05,
      "loss": 0.3831,
      "step": 28100
    },
    {
      "epoch": 55.78635014836795,
      "grad_norm": 2.0198280811309814,
      "learning_rate": 1.882182310176095e-05,
      "loss": 0.3831,
      "step": 28200
    },
    {
      "epoch": 55.984174085064296,
      "grad_norm": 2.085092067718506,
      "learning_rate": 1.6794312937258415e-05,
      "loss": 0.3978,
      "step": 28300
    },
    {
      "epoch": 56.181998021760634,
      "grad_norm": 2.2106118202209473,
      "learning_rate": 1.4881394675633541e-05,
      "loss": 0.3793,
      "step": 28400
    },
    {
      "epoch": 56.37982195845697,
      "grad_norm": 1.8325258493423462,
      "learning_rate": 1.3083291266109298e-05,
      "loss": 0.3743,
      "step": 28500
    },
    {
      "epoch": 56.57764589515331,
      "grad_norm": 1.459700584411621,
      "learning_rate": 1.1400212276321376e-05,
      "loss": 0.3826,
      "step": 28600
    },
    {
      "epoch": 56.775469831849655,
      "grad_norm": 1.9430313110351562,
      "learning_rate": 9.832353867893384e-06,
      "loss": 0.3842,
      "step": 28700
    },
    {
      "epoch": 56.97329376854599,
      "grad_norm": 1.9631510972976685,
      "learning_rate": 8.379898773574923e-06,
      "loss": 0.3856,
      "step": 28800
    },
    {
      "epoch": 57.17111770524233,
      "grad_norm": 1.6796910762786865,
      "learning_rate": 7.043016275943614e-06,
      "loss": 0.3821,
      "step": 28900
    },
    {
      "epoch": 57.368941641938676,
      "grad_norm": 2.120905637741089,
      "learning_rate": 5.821862187675775e-06,
      "loss": 0.3801,
      "step": 29000
    },
    {
      "epoch": 57.566765578635014,
      "grad_norm": 1.6467241048812866,
      "learning_rate": 4.7165788333860535e-06,
      "loss": 0.374,
      "step": 29100
    },
    {
      "epoch": 57.76458951533135,
      "grad_norm": 2.0624189376831055,
      "learning_rate": 3.7272950330400345e-06,
      "loss": 0.3858,
      "step": 29200
    },
    {
      "epoch": 57.9624134520277,
      "grad_norm": 1.6642714738845825,
      "learning_rate": 2.8541260869403564e-06,
      "loss": 0.37,
      "step": 29300
    },
    {
      "epoch": 58.160237388724035,
      "grad_norm": 2.147428274154663,
      "learning_rate": 2.0971737622883515e-06,
      "loss": 0.383,
      "step": 29400
    },
    {
      "epoch": 58.35806132542037,
      "grad_norm": 1.5963542461395264,
      "learning_rate": 1.4565262813230896e-06,
      "loss": 0.3833,
      "step": 29500
    },
    {
      "epoch": 58.55588526211672,
      "grad_norm": 2.157182455062866,
      "learning_rate": 9.322583110392691e-07,
      "loss": 0.3714,
      "step": 29600
    },
    {
      "epoch": 58.753709198813056,
      "grad_norm": 1.9097899198532104,
      "learning_rate": 5.244309544850667e-07,
      "loss": 0.3744,
      "step": 29700
    },
    {
      "epoch": 58.95153313550939,
      "grad_norm": 2.286287546157837,
      "learning_rate": 2.3309174364027906e-07,
      "loss": 0.3712,
      "step": 29800
    },
    {
      "epoch": 59.14935707220574,
      "grad_norm": 1.7390086650848389,
      "learning_rate": 5.827463387653165e-08,
      "loss": 0.366,
      "step": 29900
    },
    {
      "epoch": 59.347181008902076,
      "grad_norm": 1.899300217628479,
      "learning_rate": 0.0,
      "loss": 0.3814,
      "step": 30000
    }
  ],
  "logging_steps": 100,
  "max_steps": 30000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9785100977414144e+17,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
